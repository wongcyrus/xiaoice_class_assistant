import argparse
import json
import logging
import os
import sys
import time
import glob
import requests
from google.cloud import storage

# Add backend root to sys.path to allow imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import admin tool for creating the course
from admin_tools.manage_courses import create_or_update_course

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- HELPERS ---

def load_cdktf_outputs():
    """
    Loads backend/cdktf_outputs.json to get infrastructure details.
    """
    output_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "cdktf_outputs.json"
    )
    
    if not os.path.exists(output_path):
        return {}
        
    try:
        with open(output_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"Failed to read cdktf_outputs.json: {e}")
        return {}

def load_credentials():
    """
    Attempts to load the API key from backend/admin_tools/api_key.json.
    Returns the key string or None if not found/valid.
    """
    key_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "admin_tools",
        "api_key.json"
    )
    
    if not os.path.exists(key_path):
        return None
        
    try:
        with open(key_path, 'r') as f:
            data = json.load(f)
            # Support the format generated by create_api_key.py or simple {"api_key": "..."}
            return data.get("key_string") or data.get("api_key")
    except Exception as e:
        logger.warning(f"Failed to read api_key.json: {e}")
        return None

def upload_to_bucket(bucket_name, source_file_path, destination_blob_name):
    """
    Uploads a file to the bucket and returns the public URL.
    """
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)

        blob.upload_from_filename(source_file_path)
        
        # Construct public URL (assuming the bucket allows public access or we use the media link)
        url = blob.public_url
        logger.info(f"✅ Uploaded {source_file_path} to {destination_blob_name}. URL: {url}")
        return url
    except Exception as e:
        logger.error(f"❌ Failed to upload {source_file_path} to bucket: {e}")
        return None

def load_slides_from_json(json_path):
    """
    Parses the progress JSON to extract slide notes.
    Returns a list of dicts with 'slide_number' and 'speaker_notes'.
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        slides_dict = data.get("slides", {})
        sorted_slides = sorted(slides_dict.values(), key=lambda x: x.get("slide_index", 0))
        
        slides_data = []
        for slide in sorted_slides:
            # Some JSONs use 'note', others might use 'original_notes' if 'note' is missing/empty?
            # Based on provided files, 'note' seems populated.
            note = slide.get("note", "")
            if not note:
                 note = slide.get("original_notes", "")
                 
            slides_data.append({
                "slide_number": str(slide.get("slide_index")),
                "speaker_notes": note
            })
            
        return slides_data
    except Exception as e:
        logger.error(f"Failed to load slides from {json_path}: {e}")
        return []

# --- DEMO DATA ---

DEMO_COURSE_ID = "showcase"
DEMO_COURSE_TITLE = "Showcase"
DEMO_LANGUAGES = ["en-US", "zh-CN", "yue-HK"]

# Mapping for visual folder suffixes
LANG_VISUAL_SUFFIX_MAP = {
    "en-US": "en",
    "zh-CN": "zh-CN",
    "yue-HK": "yue-HK"
}

# -----------------

def create_demo_course():
    """Creates the demo course in Firestore using admin tools."""
    logger.info(f"Creating demo course: {DEMO_COURSE_ID}...")
    
    voice_configs = {
        "en-US": {"name": "en-US-Neural2-F", "gender": "FEMALE"},
        "zh-CN": {"name": "cmn-CN-Chirp3-HD-Achernar", "gender": "FEMALE"},
        "yue-HK": {"name": "yue-HK-Standard-A", "gender": "FEMALE"}
    }
    
    try:
        create_or_update_course(
            DEMO_COURSE_ID, 
            DEMO_COURSE_TITLE, 
            DEMO_LANGUAGES, 
            voice_configs
        )
        logger.info("✅ Course created successfully.")
    except Exception as e:
        logger.error(f"❌ Failed to create course: {e}")
        sys.exit(1)

def simulate_presentation(api_url, api_key, slides_data, ppt_filename, bucket_name=None):
    """Simulates a live presentation by sending requests to the API."""
    logger.info(f"Starting simulation for {len(slides_data)} slides from {ppt_filename}...")
    logger.info(f"Target API: {api_url}")
    
    # Append API key to URL if provided
    url = api_url
    if api_key:
        if "?" in url:
            url += f"&key={api_key}"
        else:
            url += f"?key={api_key}"

    # Determine visuals directory
    seeds_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Attempt to guess the visuals directory based on the ppt_filename
    # Assuming ppt_filename comes from backend/seeds/generate/
    # and visuals might be in backend/seeds/generate/ or similar.
    # However, original code logic was: backend/seeds/generate/{ppt_basename}_en_visuals
    # We will try to stick to a similar convention or look in the same dir.
    
    # If ppt_filename is a full path, get basename.
    ppt_basename = os.path.splitext(os.path.basename(ppt_filename))[0]
    
    # Clean up basename if it has suffixes like _with_visuals
    # Original logic was: ppt_basename of "Physics_101_Lecture_1.pptx" -> "Physics_101_Lecture_1"
    # Folder: "Physics_101_Lecture_1_en_visuals"
    
    # New files: "Physics_101_Lecture_1_en_with_visuals.pptx" -> basename "Physics_101_Lecture_1_en_with_visuals"
    # This might complicate finding the visuals folder if the naming isn't exact.
    # We'll do a best effort search for the visual file.

    for slide in slides_data:
        slide_num = slide["slide_number"]
        notes = slide["speaker_notes"]
        
        logger.info(f"\n--- Processing {os.path.basename(ppt_filename)}, Slide {slide_num} ---")
        logger.info(f"Notes: {notes[:60]}...")
        
        payload = {
            "generate_presentation": True,
            "courseId": DEMO_COURSE_ID,
            "context": notes,
            "ppt_filename": os.path.basename(ppt_filename), # Send just the filename
            "page_number": slide_num
        }

        language_specific_slide_links = {}
        if bucket_name:
            # Try to find visuals. 
            # We'll look in 'generate' folder for folders matching expected patterns
            # or just assume the visuals are not critical for this seed script if not found.
            
            # Heuristic: Try to find a folder that looks like it belongs to this PPT
            # Base prefix: e.g. "Physics_101_Lecture_1"
            
            # For now, let's try the direct pattern if it exists
            base_search_name = ppt_basename.replace("_with_visuals", "").replace("_with_notes", "").replace("_en", "").replace("_zh-CN", "").replace("_yue-HK", "")
            
            for lang_code, suffix in LANG_VISUAL_SUFFIX_MAP.items():
                # Construct potential visual directory paths
                # 1. Try {base}_{suffix}_visuals (original style)
                visuals_dir_candidates = [
                    os.path.join(seeds_dir, "generate", f"{base_search_name}_{suffix}_visuals"),
                    os.path.join(seeds_dir, "generate", f"{ppt_basename}_{suffix}_visuals"),
                    os.path.join(seeds_dir, "generate", f"{ppt_basename}_visuals"),
                ]
                
                image_filename = f"slide_{slide_num}_reimagined.png"
                
                found_image = None
                for v_dir in visuals_dir_candidates:
                    candidate_path = os.path.join(v_dir, image_filename)
                    if os.path.exists(candidate_path):
                        found_image = candidate_path
                        break
                
                if found_image:
                    logger.info(f"Found visual for {lang_code}: {image_filename}. Uploading to bucket...")
                    # Blob name: generated_visuals/{clean_basename}/{lang_code}/{filename}
                    blob_name = f"generated_visuals/{base_search_name}/{lang_code}/{image_filename}"
                    
                    image_url = upload_to_bucket(bucket_name, found_image, blob_name)
                    if image_url:
                        language_specific_slide_links[lang_code] = image_url
                        logger.info(f"Added slide_link for {lang_code}: {image_url}")
                # else:
                #     logger.warning(f"Visual image not found for {lang_code} (checked multiple paths)")
        
        if language_specific_slide_links:
            payload["language_specific_slide_links"] = language_specific_slide_links

        try:
            start_time = time.time()
            response = requests.post(url, json=payload)
            duration = time.time() - start_time
            
            if response.status_code == 200:
                logger.info(f"✅ API Success ({duration:.2f}s)")
            else:
                logger.error(f"❌ API Error {response.status_code}: {response.text}")
        except Exception as e:
            logger.error(f"❌ Request Failed: {e}")
            
        # Wait a bit between slides
        logger.info("Waiting 2 seconds before next slide...")
        time.sleep(2)

    logger.info(f"\n✅ Simulation for {ppt_filename} complete!")

def main():
    parser = argparse.ArgumentParser(description="Seed Demo Class and Simulate Presentation")
    parser.add_argument("--api-url", help="Base URL of the Config Cloud Function")
    parser.add_argument("--api-key", help="API Key for the gateway")
    parser.add_argument("--skip-create", action="store_true", help="Skip creating the course (simulation only)")
    
    args = parser.parse_args()
    
    # Resolve defaults
    file_api_key = load_credentials()
    final_api_key = args.api_key or file_api_key
    
    final_api_url = args.api_url
    bucket_name = None
    
    # Try to load from cdktf_outputs.json
    outputs = load_cdktf_outputs()
    
    # Check if outputs are nested under "cdktf" or similar
    cdktf_outputs = outputs.get("cdktf", outputs)
    
    if not final_api_url:
        base_url = cdktf_outputs.get("api-url")
        if base_url:
            # Remove trailing slash
            if base_url.endswith("/"):
                base_url = base_url[:-1]
            
            # Construct full config endpoint
            final_api_url = f"https://{base_url}/api/config"
    
    bucket_name = cdktf_outputs.get("speech-file-bucket")
    if bucket_name:
        logger.info(f"Found speech bucket: {bucket_name}")
    else:
        logger.warning("⚠️ Speech bucket not found in cdktf_outputs.json")

    if not final_api_url:
        logger.error("❌ API URL is required. Provide --api-url or ensure backend/cdktf_outputs.json exists and has 'api-url'.")
        sys.exit(1)
        
    if not final_api_key:
        logger.warning("⚠️ No API Key provided or found. Request may fail if Gateway is secured.")

    if not args.skip_create:
        create_demo_course()
    else:
        logger.info("Skipping course creation as requested.")
        
    # Find and process presentations from backend/seeds/generate/
    seeds_dir = os.path.dirname(os.path.abspath(__file__))
    generate_dir = os.path.join(seeds_dir, "generate")
    
    # Pattern to match English progress files: *_en_progress.json
    progress_files = glob.glob(os.path.join(generate_dir, "*_en_progress.json"))
    
    # Sort for consistent order
    progress_files.sort()
    
    if not progress_files:
        logger.warning(f"No *_en_progress.json files found in {generate_dir}")
        return

    for json_path in progress_files:
        logger.info(f"Found progress file: {json_path}")
        
        # Determine corresponding PPT file
        # Assume naming convention: {base}_progress.json -> {base}_with_visuals.pptm/.pptx or {base}_with_notes.pptm/.pptx
        base_name = os.path.basename(json_path).replace("_progress.json", "")
        
        # Candidates for PPT file
        ppt_candidates = [
            f"{base_name}_with_visuals.pptm",
            f"{base_name}_with_visuals.pptx"
        ]
        
        ppt_path = None
        for cand in ppt_candidates:
            cand_path = os.path.join(generate_dir, cand)
            if os.path.exists(cand_path):
                ppt_path = cand_path
                break
        
        if not ppt_path:
            logger.warning(f"Could not find matching PPT file for {json_path}. Skipping.")
            continue
            
        logger.info(f"Using PPT file: {ppt_path}")
        
        slides_data = load_slides_from_json(json_path)
        if not slides_data:
            logger.warning("No slides loaded from JSON. Skipping.")
            continue
            
        simulate_presentation(final_api_url, final_api_key, slides_data, ppt_path, bucket_name)
        
        logger.info("\n--- Pausing between presentations ---")
        time.sleep(5)

if __name__ == "__main__":
    main()
